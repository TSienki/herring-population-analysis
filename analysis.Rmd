---
title: "Projekt z analizy danych - analiza długości śledzia oceanicznego wyławianego w Europie."
author: Tomasz Sienkiewicz, Jakub Zdanowski
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Podsumowanie analizy


# Wprowadzenie
### Opis problemu
Celem projektu jest określenie głównych przyczyn stopniowego zmniejszania się długości śledzi oceanicznych wyławionych w Europie.

### Opis danych 
Zbiór danych do analizy składa się z pomiarów śledzi i warunków w jakich żyją z ostatnich 60 lat. Dane były pobierane z połowów komercyjnych jednostek. W ramach połowu jednej jednostki losowo wybierano od 50 do 100 sztuk trzyletnich śledzi.

### Opis zmiennych: 

Wszystkie dane, są danymi liczbowymi, nie występują żadne dane kategoryczne.

* __<span style="color:red">length</span>__: długość złowionego śledzia [cm];
* __cfin1__: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 1];
* __cfin2__: dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 2];
* __chel1__: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 1];
* __chel2__: dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 2];
* __lcop1__: dostępność planktonu [zagęszczenie widłonogów gat. 1];
* __lcop2__: dostępność planktonu [zagęszczenie widłonogów gat. 2];
* __fbar__: natężenie połowów w regionie [ułamek pozostawionego narybku];
* __recr__: roczny narybek [liczba śledzi];
* __cumf__: łączne roczne natężenie połowów w regionie [ułamek pozostawionego narybku];
* __totaln__: łączna liczba ryb złowionych w ramach połowu [liczba śledzi] - liczba całkowita;
* __sst__: temperatura przy powierzchni wody [°C];
* __sal__: poziom zasolenia wody [Knudsen ppt];
* __xmonth__: miesiąc połowu [numer miesiąca];
* __nao__: oscylacja północnoatlantycka [mb].

### Inicjalizacja środowiska 

```{r echo=FALSE, results='hide', message=FALSE}
library(dplyr)
library(ggplot2)
library(GGally) # ggpairplot
library(DT)
library(mice) # for missing data
library(ggpubr) # ggarange
library(corrplot) # ggarange
library(plotly)
library(caret)

columns_names <- c("length" = "długość złowionego śledzia [cm]",
  "cfin1" = "dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 1]",
  "cfin2" = "dostępność planktonu [zagęszczenie Calanus finmarchicus gat. 2]",
  "chel1" = "dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 1]",
  "chel2" = "dostępność planktonu [zagęszczenie Calanus helgolandicus gat. 2]",
  "lcop1" = "dostępność planktonu [zagęszczenie widłonogów gat. 1]",
  "lcop2" = "dostępność planktonu [zagęszczenie widłonogów gat. 2]",
  "fbar" = "natężenie połowów w regionie [ułamek pozostawionego narybku]",
  "recr" = "roczny narybek [liczba śledzi]",
  "cumf" = "łączne roczne natężenie połowów w regionie [ułamek pozostawionego narybku]",
  "totaln" = "łączna liczba ryb złowionych w ramach połowu [liczba śledzi]",
  "sst" = "temperatura przy powierzchni wody [°C]",
  "sal"  = "poziom zasolenia wody [Knudsen ppt]",
  "xmonth" = "miesiąc połowu [numer miesiąca]",
  "nao" = "oscylacja północnoatlantycka [mb]" )

prettyTable <- function(table_df, round_columns=numeric(), round_digits=2) {
    DT::datatable(table_df, style="bootstrap", filter = "top", rownames = FALSE, extensions = "Buttons", options = list(dom = 'Bfrtip', buttons = c('copy', 'csv', 'excel', 'pdf', 'print'))) %>%
    formatRound(round_columns, round_digits)
}
```



# Analiza

### Wczytanie i wstepne oczyszczenie danych

Pierwszym krokiem jest wczytanie danych oraz bliższe przyjżenie się im. Ponieważ w pliku csv brakujące rekordy zapisane są jako "?" trzeba je poprawnie wczytać.Brakujące dane występują w kolumnach cfin1, cfin2, chel1, chel2, lcop1, lcop2 i lsst. 
Jako że kolumna x to po prostu numer pomiaru, nie będzie nam potrzebna.

```{r}
set.seed(23)
data <- read.csv("sledzie.csv", na.strings = c("?"))
lapply(data, function(x) any(is.na(x)))
data <- subset(data, select = -c(X))

summary(data)
paste0("Ilość badanych przypadków: ", nrow(data), ", ilość zmiennych: ", ncol(data))
head(data, 10)
```
W celu poradzenia sobie z brakującymi danymi zostaje wykorzystana metoda MICE (Multivariate Imputation via Chained Equations). Metoda ta pozwala na pozbycie się brakujących danych z utrzymaniem podobnego rozkład

```{r cache = true}
plot_histograms <- function(data, columns=NULL, ncol=1, nrow=1, createName=function(x) x, bins=30){
  if(is.null(columns)){
    columns <- names(data)
  } 
  
  i <- 1
  plots <- list()
  for (column_name in columns){
    p <- ggplot(data, aes_string(x = column_name)) + 
      geom_histogram(color="black", fill="orange", bins=bins) +
      # geom_density() +
      ggtitle(createName(column_name)) + 
      ylab("Liczba obserwacji")
    plots[[i]] <- p
    i <- i + 1
  }
  figure <- ggarrange(plotlist = plots, ncol = ncol, nrow = nrow)
  figure
}


  

missing_data_columns <- c("cfin1", "cfin2", "chel1", "chel2", "lcop1", "lcop2", "sst")
plot_histograms(data, columns = missing_data_columns, createName = function(x) paste0("Histogram ", x, " przed pozbyciem się brakujących danych"), nrow=2)
# mice_plot <- aggr(data, col=c('navyblue','yellow'),
#                     numbers=TRUE, sortVars=TRUE,
#                     labels=names(data), cex.axis=.7,
#                     gap=3, ylab=c("Missing data","Pattern"))
completed_data <- data %>%
  mice(m=3,  method = 'cart', seed = 63) %>%
  complete(2)
plot_histograms(completed_data, columns = missing_data_columns, createName = function(x) paste0("Histogram ", x, " po pozbyciu się brakujących danych"), nrow=2)

```

### Eksploracja danych 

Podsumowanie danych
```{r}
summary(completed_data)
```

Histogramy wartości atrybutów w zbiorze danych 
Jak można zauwyć wartości długości śledzia przyjmują rozkład normalny z średnią na poziomie 25.3 cm i przyjmuje wartości w granicach 19-32.5cm. 
```{r}
plot_histograms(completed_data, createName = function(x) paste0("Histogram - ", columns_names[x]), nrow=1, bins=40)


```

Poniżej została przedstawiona korelacja pomiędzy atrybutami.
Można zauważyć, że występuje wysoka korelacja pomiędzy parami atrybutów chel1-lcop1, chel2-lcop2 oraz fbar-cumf. Korelacja może występować, ponieważ, dwie pierwsze pary związane są z dostępnością zaś ostatnia para korelacji może występować, ponieważ obydwa atrybuty związane są z natężeniem popłowu w regionie. Można te pary zredukować, aby przeciwdziałać klątwie wielowymiarowości - jednakże tutaj zostaną one zostawione bez zmian. 

```{r}
corrplot(cor(completed_data), type = "upper",  tl.col = "black", tl.srt = 45, method="number")

ggplot(completed_data, aes(x=chel1, y=lcop1)) + 
  geom_point() + 
  geom_smooth(method=lm) +
  ggtitle("Zależność pomiędzy atrybutem chel1 i lcop1")

linear_model_1 <- lm(chel1 ~ lcop1, data = completed_data)


ggplot(completed_data, aes(x=chel2, y=lcop2)) + 
  geom_point() + 
  geom_smooth(method=lm) +
  ggtitle("Zależność pomiędzy atrybutem chel2 i lcop2")
linear_model_2 <- lm(chel2 ~ lcop2, data = completed_data)


ggplot(completed_data, aes(x=fbar, y=cumf)) + 
  geom_point() + 
  geom_smooth(method=lm) +
  ggtitle("Zależność pomiędzy atrybutem fbar i cumf")
linear_model_3 <- lm(fbar ~ cumf, data = completed_data)

```

Poniżej przedstawiona została analiza długości śledzia na przestrzeni czasu. Jednakże czas nie był przedstawiony jawnie, ale wiadomo, że dane są ustawione chronologicznie dzięki czemu, zbiór można podzielić i na każdym z podziałów policzyć średnią. Można zauważyć, że na początku długość śledzia na przestrzeni czasu wzrastała od średniej wielkości w oklicach 24.5 cm do wielkości wielkości przekraczającej 27cm. Jednakże później trend się odwrócił i ostatecznie na sam koniec średnia ta była poniżej 24 cm. 
```{r}
cases_per_aggregate <- 200
aggregated_data <- aggregate(completed_data, 
                        list(rep(1:(nrow(completed_data)%/%cases_per_aggregate+1), each=cases_per_aggregate, len=nrow(completed_data))),
                        mean) # Try using mean without outliners
aggregated_data$time_period = c(1:nrow(aggregated_data))
ggplot(aggregated_data, aes(time_period, length)) +
  geom_point() + 
  geom_smooth() +
  ggtitle("Wykres zmiany długości śledzia na przestrzeni czasu") +
  ylab("Długość śledzia") +
  xlab("Przedział czasowy")

```

```{r}
plot_ly(data = aggregated_data, x = ~time_period, y = ~length,
            text = ~paste("Length: ", length)) 
# %>% layout(
#               scene = list(
#                  xaxis = list(title = "Przedział czasowy"),
#                  yaxis = list(title = "Długość śledzia")
#               )
#             )
```
# Regresor

```{r}

train_index <- createDataPartition(completed_data$length, p = .7, 
                                  list = FALSE, 
                                  times = 1)
training <- completed_data[ train_index,]
testing  <- completed_data[-train_index,]

r_squared <- function (x, y) cor(x, y) ^ 2

```

## Random Forest

It works, but without grid searching. 
```{r}
set.seed(23)

ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  search = 'random'
)


random_forest <- train(length ~ .,
             data = training,
             method = "rf",
             preProc = c("center", "scale"),
             metric = "RMSE",
             trControl = ctrl,
             ntree = 10,
             importance=TRUE,
             verbose=TRUE)

```

```{r}
evaluate_model <- function(model, test_data) {
  plot <- ggplot(varImp(model))
  print(plot)
  print(model)
  predicted <- predict(model, newdata = test_data)
  print(paste0("RMSE, na zbiorze testowym: ", RMSE(predicted, testing$length), " R-Squared na zbiorze testowym ", r_squared(predicted, test_data$length))) 
}

evaluate_model(random_forest, training)

```

It doesn't work (slow computing) :/  

```{r}
set.seed(23)
modelLookup(model='rf')

rf_grid <- expand.grid(mtry=c(2, 3, 4, 5, 6, 7, 8, 9, 11, 14))

ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
  )


random_forest <- train(length ~ .,
             data = training,
             method = "rf",
             preProc = c("center", "scale"),
             metric = "RMSE",
             trControl = ctrl,
             tuneGrid = rf_grid,
             ntree = 10,
             importance=TRUE)

evaluate_model(random_forest, training)

```


## Gradient Boosting Regressor

It doesn't work (slow computing) :/  


```{r}

modelLookup(model='gbm')

gbm_grid <- expand.grid(
  n.trees=c(10,20,50,100,500,1000),
  shrinkage=c(0.01,0.05,0.1,0.5),
  n.minobsinnode = c(3,5,10),
  interaction.depth=c(1,5,10)
  )

set.seed(23)
gbm_grid 
# gbm_grid <-  expand.grid(interaction.depth = c(1, 3, 6, 9, 10),
#                     n.trees = (0:50)*50, 
#                     shrinkage = seq(.0005, .05,.0005),
#                     n.minobsinnode = c(5, 10, 15, 20))


gbm <- train(length ~ ., 
             data = training, 
             method = "gbm", 
             metric = "RMSE",
             trControl = ctrl,
             tuneGrid = gbm_grid,
             preProc = c("center", "scale"),
             verbose = FALSE)

evaluate_model(gbm)
```

## Random Forest


# Wymagania
**Do usunięcia później**
TODO
5. Sekcję podsumowującą rozmiar zbioru i podstawowe statystyki.
9. Sekcję próbującą stworzyć regresor przewidujący rozmiar śledzia (w tej sekcji należy wykorzystać wiedzę z pozostałych punktów oraz wykonać dodatkowe czynności, które mogą poprawić trafność predykcji); dobór parametrów modelu oraz oszacowanie jego skuteczności powinny zostać wykonane za pomocą techniki podziału zbioru na dane uczące, walidujące i testowe; trafność regresji powinna zostać oszacowana na podstawie miar R2 i RMSE.
10. Analizę ważności atrybutów najlepszego znalezionego modelu regresji. Analiza ważności atrybutów powinna stanowić próbę odpowiedzi na pytanie: co sprawia, że rozmiar śledzi zaczął w pewnym momencie maleć.

Done
1. Kod wyliczający wykorzystane biblioteki.
2. Kod zapewniający powtarzalność wyników przy każdym uruchomieniu raportu na tych samych danych.
3. Kod pozwalający wczytać dane z pliku.
4. Kod przetwarzający brakujące dane.
6. Szczegółową analizę wartości atrybutów (np. poprzez prezentację rozkładów wartości).
7. Sekcję sprawdzającą korelacje między zmiennymi; sekcja ta powinna zawierać jakąś formę graficznej prezentacji korelacji.
8. Interaktywny wykres lub animację prezentującą zmianę rozmiaru śledzi w czasie.
